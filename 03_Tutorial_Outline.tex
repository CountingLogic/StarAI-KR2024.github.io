\section{Tutorial Outline}
The tutorial will be divided into three parts:

\subsubsection{Introduction} [30 Minutes] 
We will first discuss some background on relational data and probability distributions on relational structures. We will formalize a general notion of probabilistic inference on relational data, and show how it maps to WFOMC for StarAI models like Markov Logic Networks and Problog. We will provide some initial examples, where tractability and inconsistency of inference can be major hurdles in real life applications.


\subsubsection{Tractability} [60 Minutes] This session will focus on conveying the key recent results in the field of WFOMC. We will first introduce the notion of types and tables in first order logic. We will then provide a thorough introduction to WFOMC in the universally quantified fragment of first order logic with two variables, providing a closed form formula for WFOMC of any universally quantified two-variable formula \cite{Symmetric_Weighted}. We will then extend this result to admit existential quantifiers using the principle of inclusion-exclusion \cite{broeck2013,MalhotraS22}. Finally, we will extend these results to admit cardinality constraints and counting quantifiers \cite{kuzelka2020weighted}. The tutorial  will convey key algorithmic and combinatorial ideas \cite{MalhotraS22} that will bring the audience a deep understanding of the state-of-the-art  WFOMC. We will introduce key open problems in this area, ranging from scalability of existing model counters \cite{pmlr-v161-bremen21a} to fine-grain analysis of known (potentially sub-optimal) upper-bounds on complexity of WFOMC. Finally, we will give a brief overview of recent results that allow for efficient model counting in first order logic fragments, with graph-theoretic constraints \cite{LI_Tree,Malhotra2023LiftedIB}. Such constraints, such as axiomatising a relation to represent a tree, forest, a connected graph etc., significantly extend the tractable fragment of WFOMC.

\subsubsection{Consistency} [60 Minutes] We will begin by introducing recent results \cite{Projectivity_Rinaldo} that have shown  that a large variety of widely used probabilistic models, encompassing almost all of existing StarAI models \cite{Projectivity_first}, do not admit basic \emph{consistency} requirements expected of sound statistical models. The lack of such consistency conditions means that StarAI models do not admit consistency of parameter estimation, i.e., as you see more and more data, it is not true that your parameters converge to the true value of the model parameters.  Such inherent inconsistencies in StarAI models make them hard to be used across varying domain sizes \cite{DA_MLN}. We will discuss recent theoretical developments in identifying fragments of StarAI models, that admit consistency of parameter estimation \cite{ECML_PROJ,Felix_Weit}. These fragments also admit tractable inference in the strongest theoretical sense, as inference complexity is independent of the domain size. We will discuss the recently introduced rich theoretical framework for projective models \cite{ijcai2020p591}, and the recent developments in their practical implementation and applications \cite{jaeger2023a}. We will especially focus on the vast array of potential applications of these models in problems like reasoning over large scale data and random relational structure generation \cite{jaeger2023a}. Finally, we will look into recent developments in understanding generalization properties of StarAI models that do not admit consistent parameter estimation \cite{DA_MLN,chen2024understanding}, and how simple regularization and re-scaling approaches can lead to provably improved domain-size generalization in practice.

We will close the session with a discussion on open problems and potential new applications in this domain.

\section{Presenter Bio}
    Sagar Malhotra is a PostDoc at TU Wien, hosted by Prof. Thomas G{\"a}rtner. He obtained his PhD in 2023 from the university of Trento on \say{Tractability and Consistency of Probabilistic Inference in Relational Domains} under the supervision of Luciano Serafini. During his PhD he worked on novel fragments of first order logic that admit tractable WFOMC \cite{MalhotraS22,Malhotra2023LiftedIB}. He also worked on consistency of inference of Markov Logic Networks \cite{ECML_PROJ}. Recently, he has been interested in quantifying domain-size generalization of StarAI models which do not admit consistent parameter estimation \cite{chen2024understanding}.