@inproceedings{A_MLN, author = {Jain, Dominik and Barthels, Andreas and Beetz, Michael}, title = {Adaptive Markov Logic Networks: Learning Statistical Relational Models with Dynamic Parameters}, year = {2010}, isbn = {9781607506058}, publisher = {IOS Press}, address = {NLD}, abstract = {Statistical relational models, such as Markov logic networks, seek to compactly describe properties of relational domains by representing general principles about objects belonging to particular classes. Models are intended to be independent of the set of objects to which these principles can be applied, and it is assumed that the principles will soundly generalize across arbitrary sets of objects. In this paper, we point out limitations of models that seek to represent the corresponding principles with a fixed set of parameters and discuss the conditions under which the soundness of fixed parameters is indeed questionable. We propose a novel representation formalism called adaptive Markov logic networks to allow more flexible representations of relational domains, which involve parameters that are dynamically adjusted to fit the properties of an instantiation by phrasing the model's parameters as functions over attributes of the instantiation at hand. We empirically demonstrate the value of our learning and representation system on a simple but well-motivated example domain.}, booktitle = {Proceedings of the 2010 Conference on ECAI 2010: 19th European Conference on Artificial Intelligence}, pages = {937–942}, numpages = {6} }

@inproceedings{Injective_MLN,
  author    = {David Buchman and
               David Poole},
  editor    = {Blai Bonet and
               Sven Koenig},
  title     = {Representing Aggregators in Relational Probabilistic Models},
  booktitle = {Proceedings of the Twenty-Ninth {AAAI} Conference on Artificial Intelligence,
               January 25-30, 2015, Austin, Texas, {USA}},
  pages     = {3489--3495},
  publisher = {{AAAI} Press},
  year      = {2015},
  url       = {http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/10032},
  timestamp = {Wed, 10 Feb 2021 08:45:11 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/BuchmanP15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{MLN_inf,
  author    = {Parag Singla and
               Pedro M. Domingos},
  title     = {Markov Logic in Infinite Domains},
  journal   = {CoRR},
  volume    = {abs/1206.5292},
  year      = {2012},
  url       = {http://arxiv.org/abs/1206.5292},
  eprinttype = {arXiv},
  eprint    = {1206.5292},
  timestamp = {Mon, 13 Aug 2018 16:49:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1206-5292.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Felix_Weit,
  author    = {Felix Q. Weitk{\"{a}}mper},
  title     = {An Asymptotic Analysis of Probabilistic Logic Programming, with Implications
               for Expressing Projective Families of Distributions},
  journal   = {Theory Pract. Log. Program.},
  volume    = {21},
  number    = {6},
  pages     = {802--817},
  year      = {2021},
  url       = {https://doi.org/10.1017/S1471068421000314},
  doi       = {10.1017/S1471068421000314},
  timestamp = {Wed, 15 Dec 2021 10:25:55 +0100},
  biburl    = {https://dblp.org/rec/journals/tplp/Weitkamper21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
a service of  Schloss Dagstuhl - Leibniz Center for Informatics	homebrowsesearchabout

@inproceedings{DA_MLN,
  author    = {Happy Mittal and
               Ayush Bhardwaj and
               Vibhav Gogate and
               Parag Singla},
  editor    = {Kamalika Chaudhuri and
               Masashi Sugiyama},
  title     = {Domain-Size Aware Markov Logic Networks},
  booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics,
               {AISTATS} 2019, 16-18 April 2019, Naha, Okinawa, Japan},
  series    = {Proceedings of Machine Learning Research},
  volume    = {89},
  pages     = {3216--3224},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v89/mittal19a.html},
  timestamp = {Fri, 07 Jun 2019 09:03:47 +0200},
  biburl    = {https://dblp.org/rec/conf/aistats/MittalBGS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{lifted_graphical_models,
  author    = {Angelika Kimmig and
               Lilyana Mihalkova and
               Lise Getoor},
  title     = {Lifted graphical models: a survey},
  journal   = {Mach. Learn.},
  volume    = {99},
  number    = {1},
  pages     = {1--45},
  year      = {2015},
  url       = {https://doi.org/10.1007/s10994-014-5443-2},
  doi       = {10.1007/s10994-014-5443-2},
  timestamp = {Mon, 02 Mar 2020 16:28:47 +0100},
  biburl    = {https://dblp.org/rec/journals/ml/KimmigMG15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
a service of  Schloss Dagstuhl - Leibniz Center for Informatics	homebrowsesearchabout
@article{SBM_ebbe,
  author  = {Emmanuel Abbe},
  title   = {Community Detection and Stochastic Block Models: Recent Developments},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {177},
  pages   = {1-86},
  url     = {http://jmlr.org/papers/v18/16-480.html}
}
a service of  Schloss Dagstuhl - Leibniz Center for Informatics	homebrowsesearchabout



@inproceedings{Population_Extrapolation,
  author    = {David Poole and
               David Buchman and
               Seyed Mehran Kazemi and
               Kristian Kersting and
               Sriraam Natarajan},
  editor    = {Umberto Straccia and
               Andrea Cal{\`{\i}}},
  title     = {Population Size Extrapolation in Relational Probabilistic Modelling},
  booktitle = {Scalable Uncertainty Management - 8th International Conference, {SUM}
               2014, Oxford, UK, September 15-17, 2014. Proceedings},
  series    = {Lecture Notes in Computer Science},
  volume    = {8720},
  pages     = {292--305},
  publisher = {Springer},
  year      = {2014},
  url       = {https://doi.org/10.1007/978-3-319-11508-5\_25},
  doi       = {10.1007/978-3-319-11508-5\_25},
  timestamp = {Tue, 14 May 2019 10:00:52 +0200},
  biburl    = {https://dblp.org/rec/conf/sum/PooleBKKN14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{MalhotraS22,
  author       = {Sagar Malhotra and
                  Luciano Serafini},
  title        = {Weighted Model Counting in {FO2} with Cardinality Constraints and
                  Counting Quantifiers: {A} Closed Form Formula},
  booktitle    = {Thirty-Sixth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2022, Thirty-Fourth Conference on Innovative Applications of Artificial
                  Intelligence, {IAAI} 2022, The Twelveth Symposium on Educational Advances
                  in Artificial Intelligence, {EAAI} 2022 Virtual Event, February 22
                  - March 1, 2022},
  pages        = {5817--5824},
  publisher    = {{AAAI} Press},
  year         = {2022},
  url          = {https://doi.org/10.1609/aaai.v36i5.20525},
  doi          = {10.1609/AAAI.V36I5.20525},
  timestamp    = {Sat, 21 Oct 2023 10:46:21 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/MalhotraS22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Proj_TOM,
author = { TOM A. B.   SNIJDERS },
title = {Conditional Marginalization for Exponential Random Graph Models},
journal = {The Journal of Mathematical Sociology},
volume = {34},
number = {4},
pages = {239-252},
year  = {2010},
publisher = {Routledge},
doi = {10.1080/0022250X.2010.485707},

URL = { 
        https://www.tandfonline.com/doi/abs/10.1080/0022250X.2010.485707
    
},
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1080/0022250X.2010.485707
    
}

}

@article{MAR,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2335739},
 abstract = {When making sampling distribution inferences about the parameter of the data, θ, it is appropriate to ignore the process that causes missing data if the missing data are `missing at random' and the observed data are `observed at random', but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about θ, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is `distinct' from θ. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
 author = {Donald B. Rubin},
 journal = {Biometrika},
 number = {3},
 pages = {581--592},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Inference and Missing Data},
 volume = {63},
 year = {1976}
}
@article{social_Network_sample,
author = {Mark S. Handcock and Krista J. Gile},
title = {{Modeling social networks from sampled data}},
volume = {4},
journal = {The Annals of Applied Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {5 -- 25},
keywords = {design-based inference, Exponential family random graph model, Markov chain Monte Carlo, p* model},
year = {2010},
doi = {10.1214/08-AOAS221},
URL = {https://doi.org/10.1214/08-AOAS221}
}

@article{social_Network_missing,
title = {Effects of missing data in social networks},
journal = {Social Networks},
volume = {28},
number = {3},
pages = {247-268},
year = {2006},
issn = {0378-8733},
doi = {https://doi.org/10.1016/j.socnet.2005.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0378873305000511},
author = {Gueorgi Kossinets},
keywords = {Missing data, Sensitivity analysis, Graph theory, Collaboration networks, Bipartite graphs},
abstract = {We perform sensitivity analyses to assess the impact of missing data on the structural properties of social networks. The social network is conceived of as being generated by a bipartite graph, in which actors are linked together via multiple interaction contexts or affiliations. We discuss three principal missing data mechanisms: network boundary specification (non-inclusion of actors or affiliations), survey non-response, and censoring by vertex degree (fixed choice design), examining their impact on the scientific collaboration network from the Los Alamos E-print Archive as well as random bipartite graphs. The simulation results show that network boundary specification and fixed choice designs can dramatically alter estimates of network-level statistics. The observed clustering and assortativity coefficients are overestimated via omission of affiliations or fixed choice thereof, and underestimated via actor non-response, which results in inflated measurement error. We also find that social networks with multiple interaction contexts may have certain interesting properties due to the presence of overlapping cliques. In particular, assortativity by degree does not necessarily improve network robustness to random omission of nodes as predicted by current theory.}
}
@misc{test_disease,
      title={Dynamic group testing to control and monitor disease progression in a population}, 
      author={Sundara Rajan Srinivasavaradhan and Pavlos Nikolopoulos and Christina Fragouli and Suhas Diggavi},
      year={2021},
      eprint={2106.10765},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}
@inproceedings{one_Network,
  author    = {Rongjing Xiang and
               Jennifer Neville},
  editor    = {Geoffrey J. Gordon and
               David B. Dunson and
               Miroslav Dud{\'{\i}}k},
  title     = {Relational Learning with One Network: An Asymptotic Analysis},
  booktitle = {Proceedings of the Fourteenth International Conference on Artificial
               Intelligence and Statistics, {AISTATS} 2011, Fort Lauderdale, USA,
               April 11-13, 2011},
  series    = {{JMLR} Proceedings},
  volume    = {15},
  pages     = {779--788},
  publisher = {JMLR.org},
  year      = {2011},
  url       = {http://proceedings.mlr.press/v15/xiang11a/xiang11a.pdf},
  timestamp = {Wed, 29 May 2019 08:41:47 +0200},
  biburl    = {https://dblp.org/rec/journals/jmlr/XiangN11.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Relational_Marginal_Polytope,
  author    = {Ondrej Kuzelka and
               Yuyi Wang and
               Jesse Davis and
               Steven Schockaert},
  title     = {Relational Marginal Problems: Theory and Estimation},
  journal   = {CoRR},
  volume    = {abs/1709.05825},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.05825},
  eprinttype = {arXiv},
  eprint    = {1709.05825},
  timestamp = {Fri, 07 Jun 2019 13:51:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1709-05825.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
a service of  Schloss Dagstuhl - Leibniz Center for Informatics	homebrowsesearchabout


@InProceedings{Kuzelka_Weight_Learning,
  title = 	 {Lifted Weight Learning of Markov Logic Networks  (Revisited One More Time)},
  author =       {Kuzelka, Ondrej and Kungurtsev, Vyacheslav and Wang, Yuyi},
  booktitle = 	 {Proceedings of the 10th International Conference on Probabilistic Graphical Models},
  pages = 	 {269--280},
  year = 	 {2020},
  editor = 	 {Jaeger, Manfred and Nielsen, Thomas Dyhre},
  volume = 	 {138},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--25 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v138/kuzelka20a/kuzelka20a.pdf},
  url = 	 {https://proceedings.mlr.press/v138/kuzelka20a.html},
  abstract = 	 {We revisit the problem of lifted weight learning of Markov logic networks (MLNs). We show that there is an algorithm for maximum-likelihood learning which runs in time polynomial in the size of the domain, whenever the partition function of the given MLN can be computed in polynomial time. This improves on our recent results where we showed the same result with the additional dependency of the runtime on a parameter of the training data, called interiority, which measures how “extreme” the given training data are. In this work, we get rid of this dependency. The main new technical ingredient that we exploit are theoretical results obtained recently by Straszak and Vishnoi (Maximum Entropy Distributions: Bit Complexity and Stability, COLT 2019).}
}

@article{Projectivity_first,
  author    = {Manfred Jaeger and
               Oliver Schulte},
  title     = {Inference, Learning, and Population Size: Projectivity for {SRL} Models},
  journal   = {CoRR},
  volume    = {abs/1807.00564},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.00564},
  eprinttype = {arXiv},
  eprint    = {1807.00564},
  timestamp = {Mon, 13 Aug 2018 16:46:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-00564.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@book{SRL_LISA,
author = {Getoor, Lise and Taskar, Ben}, 
title = {Introduction to Statistical Relational Learning (Adaptive Computation and Machine Learning)}, 
year = {2007}, 
isbn = {0262072882}, 
publisher = {The MIT Press} }

@article{Projectivity_Rinaldo,
  title={CONSISTENCY UNDER SAMPLING OF EXPONENTIAL RANDOM GRAPH MODELS.},
  author={Cosma Rohilla Shalizi and Alessandro Rinaldo},
  journal={Annals of statistics},
  year={2013},
  volume={41 2},
  pages={
          508-535
        }
}
@inproceedings{Projectivity_SRL,
  author    = {Manfred Jaeger and
               Oliver Schulte},
  editor    = {Christian Bessiere},
  title     = {A Complete Characterization of Projectivity for Statistical Relational
               Models},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI} 2020},
  pages     = {4283--4290},
  publisher = {ijcai.org},
  year      = {2020},
  url       = {https://doi.org/10.24963/ijcai.2020/591},
  doi       = {10.24963/ijcai.2020/591},
  timestamp = {Mon, 20 Jul 2020 12:38:52 +0200},
  biburl    = {https://dblp.org/rec/conf/ijcai/JaegerS20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{domain_lifted_defintion,
 author = {Guy Van den Broeck},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On the Completeness of First-Order Knowledge Compilation for Lifted Probabilistic Inference},
 url = {https://proceedings.neurips.cc/paper/2011/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf},
 volume = {24},
 year = {2011}
}


@article{SBM,
title = {Stochastic blockmodels: First steps},
journal = {Social Networks},
volume = {5},
number = {2},
pages = {109-137},
year = {1983},
issn = {0378-8733},
doi = {https://doi.org/10.1016/0378-8733(83)90021-7},
url = {https://www.sciencedirect.com/science/article/pii/0378873383900217},
author = {Paul W. Holland and Kathryn Blackmond Laskey and Samuel Leinhardt},
abstract = {A stochastic model is proposed for social networks in which the actors in a network are partitioned into subgroups called blocks. The model provides a stochastic generalization of the blockmodel. Estimation techniques are developed for the special case of a single relation social network, with blocks specified a priori. An extension of the model allows for tendencies toward reciprocation of ties beyond those explained by the partition. The extended model provides a one degree-of-freedom test of the model. A numerical example from the social network literature is used to illustrate the methods.}
}



@inproceedings{First_Order_Prob_Inf,
author    = {David Poole},
  editor    = {Georg Gottlob and
               Toby Walsh},
  title     = {First-order probabilistic inference},
  booktitle = {IJCAI-03, Proceedings of the Eighteenth International Joint Conference
               on Artificial Intelligence, Acapulco, Mexico, August 9-15, 2003},
  pages     = {985--991},
  publisher = {Morgan Kaufmann},
  year      = {2003},
  url       = {http://ijcai.org/Proceedings/03/Papers/142.pdf},
  timestamp = {Tue, 20 Aug 2019 16:19:14 +0200},
  biburl    = {https://dblp.org/rec/conf/ijcai/Poole03.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{de_Salvo,
author    = {Rodrigo de Salvo Braz and
               Eyal Amir and
               Dan Roth},
  editor    = {Leslie Pack Kaelbling and
               Alessandro Saffiotti},
  title     = {Lifted First-Order Probabilistic Inference},
  booktitle = {IJCAI-05, Proceedings of the Nineteenth International Joint Conference
               on Artificial Intelligence, Edinburgh, Scotland, UK, July 30 - August
               5, 2005},
  pages     = {1319--1325},
  publisher = {Professional Book Center},
  year      = {2005},
  url       = {http://ijcai.org/Proceedings/05/Papers/1548.pdf},
  timestamp = {Tue, 20 Aug 2019 16:17:40 +0200},
  biburl    = {https://dblp.org/rec/conf/ijcai/BrazAR05.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{CHAVIRA2008772,
 author    = {Mark Chavira and
               Adnan Darwiche},
  title     = {On probabilistic inference by weighted model counting},
  journal   = {Artif. Intell.},
  volume    = {172},
  number    = {6-7},
  pages     = {772--799},
  year      = {2008},
  url       = {https://doi.org/10.1016/j.artint.2007.11.002},
  doi       = {10.1016/j.artint.2007.11.002},
  timestamp = {Sat, 27 May 2017 14:24:42 +0200},
  biburl    = {https://dblp.org/rec/journals/ai/ChaviraD08.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{LIFTED_PI_KB_COMPLETION,
author    = {Guy Van den Broeck and
               Nima Taghipour and
               Wannes Meert and
               Jesse Davis and
               Luc De Raedt},
  editor    = {Toby Walsh},
  title     = {Lifted Probabilistic Inference by First-Order Knowledge Compilation},
  booktitle = {{IJCAI} 2011, Proceedings of the 22nd International Joint Conference
               on Artificial Intelligence, Barcelona, Catalonia, Spain, July 16-22,
               2011},
  pages     = {2178--2185},
  publisher = {{IJCAI/AAAI}},
  year      = {2011},
  url       = {https://doi.org/10.5591/978-1-57735-516-8/IJCAI11-363},
  doi       = {10.5591/978-1-57735-516-8/IJCAI11-363},
  timestamp = {Sun, 25 Oct 2020 23:13:55 +0100},
  biburl    = {https://dblp.org/rec/conf/ijcai/BroeckTMDR11.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{broeck2013,
author    = {Guy Van den Broeck and
               Wannes Meert and
               Adnan Darwiche},
  editor    = {Chitta Baral and
               Giuseppe De Giacomo and
               Thomas Eiter},
  title     = {Skolemization for Weighted First-Order Model Counting},
  booktitle = {Principles of Knowledge Representation and Reasoning: Proceedings
               of the Fourteenth International Conference, {KR} 2014, Vienna, Austria,
               July 20-24, 2014},
  publisher = {{AAAI} Press},
  year      = {2014},
  url       = {http://www.aaai.org/ocs/index.php/KR/KR14/paper/view/8012},
  timestamp = {Tue, 09 Feb 2021 08:33:43 +0100},
  biburl    = {https://dblp.org/rec/conf/kr/BroeckMD14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
a service of  Schloss Dagstuhl - Leibniz Center for Informatics	homebrowsesearchabout



@inproceedings{kazemi2016new,
 author    = {Seyed Mehran Kazemi and
               Angelika Kimmig and
               Guy Van den Broeck and
               David Poole},
  editor    = {Daniel D. Lee and
               Masashi Sugiyama and
               Ulrike von Luxburg and
               Isabelle Guyon and
               Roman Garnett},
  title     = {New Liftable Classes for First-Order Probabilistic Inference},
  booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference
               on Neural Information Processing Systems 2016, December 5-10, 2016,
               Barcelona, Spain},
  pages     = {3117--3125},
  year      = {2016},
  url       = {https://proceedings.neurips.cc/paper/2016/hash/c88d8d0a6097754525e02c2246d8d27f-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:22 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/KazemiKBP16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Symmetric_Weighted,
 author    = {Paul Beame and
               Guy Van den Broeck and
               Eric Gribkoff and
               Dan Suciu},
  editor    = {Tova Milo and
               Diego Calvanese},
  title     = {Symmetric Weighted First-Order Model Counting},
  booktitle = {Proceedings of the 34th {ACM} Symposium on Principles of Database
               Systems, {PODS} 2015, Melbourne, Victoria, Australia, May 31 - June
               4, 2015},
  pages     = {313--328},
  publisher = {{ACM}},
  year      = {2015},
  url       = {https://doi.org/10.1145/2745754.2745760},
  doi       = {10.1145/2745754.2745760},
  timestamp = {Tue, 06 Nov 2018 16:58:02 +0100},
  biburl    = {https://dblp.org/rec/conf/pods/BeameBGS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{AND_JOIN,
author    = {Murray Patterson and
               Yongmei Liu and
               Eugenia Ternovska and
               Arvind Gupta},
  editor    = {Manuela M. Veloso},
  title     = {Grounding for Model Expansion in k-Guarded Formulas with Inductive
               Definitions},
  booktitle = {{IJCAI} 2007, Proceedings of the 20th International Joint Conference
               on Artificial Intelligence, Hyderabad, India, January 6-12, 2007},
  pages     = {161--166},
  year      = {2007},
  url       = {http://ijcai.org/Proceedings/07/Papers/024.pdf},
  timestamp = {Tue, 20 Aug 2019 16:19:13 +0200},
  biburl    = {https://dblp.org/rec/conf/ijcai/PattersonLTG07.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
a service of  Schloss Dagstuhl - Leibniz Center for Informatics	homebrowsesearchabout

@article{Scott1962,
  added-at = {2012-07-01T20:20:30.000+0200},
  author = {Scott, D.},
  biburl = {https://www.bibsonomy.org/bibtex/25572864e3488fa2afe883dd9916cf261/ralenda},
  groups = {public},
  interhash = {7d64798910e8df0ceed629743f4b5a54},
  intrahash = {5572864e3488fa2afe883dd9916cf261},
  journal = {Journal of Symbolic Logic},
  keywords = {},
  pages = 377,
  timestamp = {2012-07-01T20:20:30.000+0200},
  title = {A decision method for validity of sentences in two variables},
  username = {ralenda},
  volume = 27,
  year = 1962
}






@phdthesis{VdBThesis13,
 author    = {Guy Van den Broeck},
  title     = {Lifted Inference and Learning in Statistical Relational Models (Eerste-orde
               inferentie en leren in statistische relationele modellen)},
  school    = {Katholieke Universiteit Leuven, Belgium},
  year      = {2013},
  url       = {https://lirias.kuleuven.be/handle/123456789/373041},
  timestamp = {Thu, 12 Mar 2020 11:23:58 +0100},
  biburl    = {https://dblp.org/rec/phd/basesearch/VandenBroeck13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{kazemi2017domain,
    title={Domain recursion for lifted inference with existential quantifiers},
  author={Kazemi, Seyed Mehran and Kimmig, Angelika and Broeck, Guy Van den and Poole, David},
  journal={arXiv preprint arXiv:1707.07763},
  year={2017}
}

@article{DOMAIN_RECURSION,
author    = {Seyed Mehran Kazemi and
               Angelika Kimmig and
               Guy Van den Broeck and
               David Poole},
  title     = {Domain Recursion for Lifted Inference with Existential Quantifiers},
  journal   = {CoRR},
  volume    = {abs/1707.07763},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.07763},
  eprinttype = {arXiv},
  eprint    = {1707.07763},
  timestamp = {Mon, 13 Aug 2018 16:48:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KazemiKBP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{kuusisto2018weighted,
author    = {Antti Kuusisto and
               Carsten Lutz},
  editor    = {Anuj Dawar and
               Erich Gr{\"{a}}del},
  title     = {Weighted model counting beyond two-variable logic},
  booktitle = {Proceedings of the 33rd Annual {ACM/IEEE} Symposium on Logic in Computer
               Science, {LICS} 2018, Oxford, UK, July 09-12, 2018},
  pages     = {619--628},
  publisher = {{ACM}},
  year      = {2018},
  url       = {https://doi.org/10.1145/3209108.3209168},
  doi       = {10.1145/3209108.3209168},
  timestamp = {Sat, 19 Oct 2019 20:00:56 +0200},
  biburl    = {https://dblp.org/rec/conf/lics/KuusistoL18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kuzelka2020weighted,
   author    = {Ondrej Kuzelka},
  title     = {Weighted First-Order Model Counting in the Two-Variable Fragment With
               Counting Quantifiers},
  journal   = {J. Artif. Intell. Res.},
  volume    = {70},
  pages     = {1281--1307},
  year      = {2021},
  url       = {https://doi.org/10.1613/jair.1.12320},
  doi       = {10.1613/jair.1.12320},
  timestamp = {Tue, 13 Apr 2021 16:07:23 +0200},
  biburl    = {https://dblp.org/rec/journals/jair/Kuzelka21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
} 

%a service of  Schloss Dagstuhl - Leibniz Center for Informatics	homebrowsesearchabout

@paper{E_LI_AF,
	author    = {Jaesik Choi and
               Rodrigo de Salvo Braz and
               Hung Hai Bui},
  editor    = {Wolfram Burgard and
               Dan Roth},
  title     = {Efficient Methods for Lifted Inference with Aggregate Factors},
  booktitle = {Proceedings of the Twenty-Fifth {AAAI} Conference on Artificial Intelligence,
               {AAAI} 2011, San Francisco, California, USA, August 7-11, 2011},
  publisher = {{AAAI} Press},
  year      = {2011},
  url       = {http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/view/3797},
  timestamp = {Thu, 18 Feb 2021 14:57:19 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/ChoiBB11.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{darwiche2002knowledge,
  title={A knowledge compilation map},
  author={Darwiche, Adnan and Marquis, Pierre},
  journal={Journal of Artificial Intelligence Research},
  volume={17},
  pages={229--264},
  year={2002}
}

@inproceedings{kieronski2015uniform,
  author    = {Antti Kuusisto},
  editor    = {Maurizio Lenzerini and
               Rafael Pe{\~{n}}aloza},
  title     = {On the Uniform One-dimensional Fragment},
  booktitle = {Proceedings of the 29th International Workshop on Description Logics,
               Cape Town, South Africa, April 22-25, 2016},
  series    = {{CEUR} Workshop Proceedings},
  volume    = {1577},
  publisher = {CEUR-WS.org},
  year      = {2016},
  url       = {http://ceur-ws.org/Vol-1577/paper\_16.pdf},
  timestamp = {Wed, 12 Feb 2020 16:44:56 +0100},
  biburl    = {https://dblp.org/rec/conf/dlog/Kuusisto16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{complexMLNkuzelka2020,
author    = {Ondrej Kuzelka},
  editor    = {Ryan P. Adams and
               Vibhav Gogate},
  title     = {Complex Markov Logic Networks: Expressivity and Liftability},
  booktitle = {Proceedings of the Thirty-Sixth Conference on Uncertainty in Artificial
               Intelligence, {UAI} 2020, virtual online, August 3-6, 2020},
  series    = {Proceedings of Machine Learning Research},
  volume    = {124},
  pages     = {729--738},
  publisher = {{AUAI} Press},
  year      = {2020},
  url       = {http://proceedings.mlr.press/v124/kuzelka20a.html},
  timestamp = {Wed, 16 Dec 2020 16:53:24 +0100},
  biburl    = {https://dblp.org/rec/conf/uai/Kuzelka20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{chavira2008probabilistic,
  title={On probabilistic inference by weighted model counting},
  author={Chavira, Mark and Darwiche, Adnan},
  journal={Artificial Intelligence},
  volume={172},
  number={6-7},
  pages={772--799},
  year={2008},
  publisher={Elsevier}
}

@article{liftedGraphicalModelsSurvew2015,
author = {Kimmig, Angelika and Mihalkova, Lilyana and Getoor, Lise},
title = {Lifted Graphical Models: A Survey},
year = {2015},
issue_date = {April     2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {99},
number = {1},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-014-5443-2},
doi = {10.1007/s10994-014-5443-2},
abstract = {Lifted graphical models provide a language for expressing dependencies between different types of entities, their attributes, and their diverse relations, as well as techniques for probabilistic reasoning in such multi-relational domains. In this survey, we review a general form for a lifted graphical model, a par-factor graph, and show how a number of existing statistical relational representations map to this formalism. We discuss inference algorithms, including lifted inference algorithms, that efficiently compute the answers to probabilistic queries over such models. We also review work in learning lifted graphical models from data. There is a growing need for statistical relational models (whether they go by that name or another), as we are inundated with data which is a mix of structured and unstructured, with entities and relations extracted in a noisy manner from text, and with the need to reason effectively with this data. We hope that this synthesis of ideas from many different research groups will provide an accessible starting point for new researchers in this expanding field.},
journal = {Mach. Learn.},
month = apr,
pages = {1–45},
numpages = {45},
keywords = {Probabilistic programming, Templated graphical models, First-order probabilistic models, Par-factor graphs, Statistical relational learning, Lifted inference and learning}
}
@article{GRADEL199973,
title = "On logics with two variables",
journal = "Theoretical Computer Science",
volume = "224",
number = "1",
pages = "73 - 113",
year = "1999",
issn = "0304-3975",
doi = "https://doi.org/10.1016/S0304-3975(98)00308-9",
url = "http://www.sciencedirect.com/science/article/pii/S0304397598003089",
author = "Erich Grädel and Martin Otto",
keywords = "Two-variable logics, Modal logics, Satisfiability, Model checking, Decidability",
abstract = "This paper is a survey and systematic presentation of decidability and complexity issues for modal and non-modal two-variable logics. A classical result due to Mortimer says that the two-variable fragment of first-order logic, denoted FO2, has the finite model property and is therefore decidable for satisfiability. One of the reasons for the significance of this result is that many propositional modal logics can be embedded into FO2. Logics that are of interest for knowledge representation, for the specification and verification of concurrent systems and for other areas of computer science are often defined (or can be viewed) as extensions of modal logics by features like counting constructs, path quantifiers, transitive closure operators, least and greatest fixed points, etc. Examples of such logics are computation tree logic CTL, the modal μ-calculus Lμ, or popular description logics used in artificial intelligence. Although the additional features are usually not first-order constructs, the resulting logics can still be seen as two-variable logics that are embedded in suitable extensions of FO2. Typically, the applications call for an analysis of the satisfiability and model checking problems of the logics employed. The decidability and complexity issues for modal and non-modal two-variables logics have been studied quite intensively in the last years. It has turned out that the satisfiability problems for two-variable logics with full first-order quantification are usually much harder (and indeed highly undecidable in many cases) than the satisfiability problems for corresponding modal logics. On the other side, the situation is different for model checking problems. The model checking problem of a modal logic has essentially the same complexity as the model checking problem of the corresponding two variable logic with full quantification."
}

@BOOK{SRL_LUC,
  author    = {Luc De Raedt and
               Kristian Kersting and
               Sriraam Natarajan and
               David Poole},
  title     = {Statistical Relational Artificial Intelligence: Logic, Probability,
               and Computation},
  series    = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  publisher = {Morgan {\&} Claypool Publishers},
  year      = {2016},
  url       = {https://doi.org/10.2200/S00692ED1V01Y201601AIM032},
  doi       = {10.2200/S00692ED1V01Y201601AIM032},
  timestamp = {Mon, 26 Oct 2020 08:19:26 +0100},
  biburl    = {https://dblp.org/rec/series/synthesis/2016Raedt.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
 @inproceedings{Problog,
author = {De Raedt, Luc and Kimmig, Angelika and Toivonen, Hannu},
title = {ProbLog: A Probabilistic Prolog and Its Application in Link Discovery},
year = {2007},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We introduce ProbLog, a probabilistic extension of Prolog. A ProbLog program defines a distribution over logic programs by specifying for each clause the probability that it belongs to a randomly sampled program, and these probabilities are mutually independent. The semantics of ProbLog is then defined by the success probability of a query, which corresponds to the probability that the query succeeds in a randomly sampled program. The key contribution of this paper is the introduction of an effective solver for computing success probabilities. It essentially combines SLD-resolution with methods for computing the probability of Boolean formulae. Our implementation further employs an approximation algorithm that combines iterative deepening with binary decision diagrams. We report on experiments in the context of discovering links in real biological networks, a demonstration of the practical usefulness of the approach.},
booktitle = {Proceedings of the 20th International Joint Conference on Artifical Intelligence},
pages = {2468–2473},
numpages = {6},
location = {Hyderabad, India},
series = {IJCAI'07}
}


@inproceedings{Problog_WMC,
abstract = {Copyright © 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). Over the last decade, building on advances in the areas of knowledge compilation and weighted model counting has drastically increased the scalability of inference in probabilistic logic programs. In this paper, we provide an overview of how this has been possible and point out some open challenges.},
journal = {Proceedings of the First Workshop on Beyond NP},
pages = {359--364},
volume = {WS-16-01 - WS-16-15},
isbn = {9781577357599},
year = {2016},
title = {Knowledge compilation and weighted model counting for inference in probabilistic logic programs},
language = {eng},
author = {Vlasselaer, Jonas and Kimmig, Angelika and Dries, Anton and Meert, Wannes and De Raedt, Luc},
url = {$$Uhttps://lirias.kuleuven.be/retrieve/353364$$DbeyondNP_Vlasselaer.pdf [freely available]},
}

@article{morettin2019advanced,
  title={Advanced smt techniques for weighted model integration},
  author={Morettin, Paolo and Passerini, Andrea and Sebastiani, Roberto},
  journal={Artificial Intelligence},
  volume={275},
  pages={1--27},
  year={2019},
  publisher={Elsevier}
}

@article{richardson2006markov,
  title={Markov logic networks},
  author={Richardson, Matthew and Domingos, Pedro},
  journal={Machine learning},
  volume={62},
  number={1-2},
  pages={107--136},
  year={2006},
  publisher={Springer}
}

@INPROCEEDINGS {COUNTING_REF,
title={Two-variable logic with counting is decidable},
  author={Gradel, Erich and Otto, Martin and Rosen, Eric},
  booktitle={Proceedings of Twelfth Annual IEEE Symposium on Logic in Computer Science},
  pages={306--317},
  year={1997},
  organization={IEEE}
}
@misc{das2016brief,
  title={A brief note on estimates of binomial coefficients},
  author={Das, Shagnik},
  year={2016}
}

@inproceedings{PTP,
  author    = {Vibhav Gogate and
               Pedro M. Domingos},
  editor    = {F{\'{a}}bio Gagliardi Cozman and
               Avi Pfeffer},
  title     = {Probabilistic Theorem Proving},
  booktitle = {{UAI} 2011, Proceedings of the Twenty-Seventh Conference on Uncertainty
               in Artificial Intelligence, Barcelona, Spain, July 14-17, 2011},
  pages     = {256--265},
  publisher = {{AUAI} Press},
  year      = {2011},
  url       = {https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1\&smnu=2\&article\_id=2263\&proceeding\_id=27},
  timestamp = {Wed, 03 Feb 2021 11:09:41 +0100},
  biburl    = {https://dblp.org/rec/conf/uai/GogateD11a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@book{GF_book,
title={Generatingfunctionology},
  author={Wilf, Herbert S},
  year={2005},
  publisher={CRC press}
}

@inproceedings{singla2008,
author = {Singla, Parag and Domingos, Pedro},
title = {Lifted first-order belief propagation},
year = {2008},
isbn = {9781577353683},
publisher = {AAAI Press},
abstract = {Unifying first-order logic and probability is a long-standing goal of AI, and in recent years many representations combining aspects of the two have been proposed. However, inference in them is generally still at the level of propositional logic, creating all ground atoms and formulas and applying standard probabilistic inference methods to the resulting network. Ideally, inference should be lifted as in first-order logic, handling whole sets of indistinguishable objects together, in time independent of their cardinality. Poole (2003) and Braz et al. (2005, 2006) developed a lifted version of the variable elimination algorithm, but it is extremely complex, generally does not scale to realistic domains, and has only been applied to very small artificial problems. In this paper we propose the first lifted version of a scalable probabilistic inference algorithm, belief propagation (loopy or not). Our approach is based on first constructing a lifted network, where each node represents a set of ground atoms that all pass the same messages during belief propagation. We then run belief propagation on this network. We prove the correctness and optimality of our algorithm. Experiments show that it can greatly reduce the cost of inference.},
booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 2},
pages = {1094–1099},
numpages = {6},
location = {Chicago, Illinois},
series = {AAAI'08}
}

@inproceedings{mihalkova2007,
author = {Mihalkova, Lilyana and Mooney, Raymond J.},
title = {Bottom-up learning of Markov logic network structure},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273575},
doi = {10.1145/1273496.1273575},
abstract = {Markov logic networks (MLNs) are a statistical relational model that consists of weighted firstorder clauses and generalizes first-order logic and Markov networks. The current state-of-the-art algorithm for learning MLN structure follows a top-down paradigm where many potential candidate structures are systematically generated without considering the data and then evaluated using a statistical measure of their fit to the data. Even though this existing algorithm outperforms an impressive array of benchmarks, its greedy search is susceptible to local maxima or plateaus. We present a novel algorithm for learning MLN structure that follows a more bottom-up approach to address this problem. Our algorithm uses a "propositional" Markov network learning method to construct "template" networks that guide the construction of candidate clauses. Our algorithm significantly improves accuracy and learning time over the existing topdown approach in three real-world domains.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {625–632},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@Article{craven2001,
author={Craven, Mark
and Slattery, Se{\'a}n},
title={Relational Learning with Statistical Predicate Invention: Better Models for Hypertext},
journal={Machine Learning},
year={2001},
month={Apr},
day={01},
volume={43},
number={1},
pages={97-119},
abstract={We present a new approach to learning hypertext classifiers that combines a statistical text-learning method with a relational rule learner. This approach is well suited to learning in hypertext domains because its statistical component allows it to characterize text in terms of word frequencies, whereas its relational component is able to describe how neighboring documents are related to each other by hyperlinks that connect them. We evaluate our approach by applying it to tasks that involve learning definitions for (i) classes of pages, (ii) particular relations that exist between pairs of pages, and (iii) locating a particular class of information in the internal structure of pages. Our experiments demonstrate that this new approach is able to learn more accurate classifiers than either of its constituent methods alone.},
issn={1573-0565},
doi={10.1023/A:1007676901476},
url={https://doi.org/10.1023/A:1007676901476}
}

@Misc{rummel1992,
author={Rummel, Rudolph J.},
title={Dimensionality of Nations Project: Attributes of Nations and Behavior of Nation Dyads, 1950-1965},
year={1992},
publisher={[distributor]},
doi={10.3886/ICPSR05409.v1},
url={https://doi.org/10.3886/ICPSR05409.v1}
}

@Article{vanHaaren2016,
author={Van Haaren, Jan
and Van den Broeck, Guy
and Meert, Wannes
and Davis, Jesse},
title={Lifted generative learning of Markov logic networks},
journal={Machine Learning},
year={2016},
month={Apr},
day={01},
volume={103},
number={1},
pages={27-55},
abstract={Markov logic networks (MLNs) are a well-known statistical relational learning formalism that combines Markov networks with first-order logic. MLNs attach weights to formulas in first-order logic. Learning MLNs from data is a challenging task as it requires searching through the huge space of possible theories. Additionally, evaluating a theory's likelihood requires learning the weight of all formulas in the theory. This in turn requires performing probabilistic inference, which, in general, is intractable in MLNs. Lifted inference speeds up probabilistic inference by exploiting symmetries in a model. We explore how to use lifted inference when learning MLNs. Specifically, we investigate generative learning where the goal is to maximize the likelihood of the model given the data. First, we provide a generic algorithm for learning maximum likelihood weights that works with any exact lifted inference approach. In contrast, most existing approaches optimize approximate measures such as the pseudo-likelihood. Second, we provide a concrete parameter learning algorithm based on first-order knowledge compilation. Third, we propose a structure learning algorithm that learns liftable MLNs, which is the first MLN structure learning algorithm that exactly optimizes the likelihood of the model. Finally, we perform an empirical evaluation on three real-world datasets. Our parameter learning algorithm results in more accurate models than several competing approximate approaches. It learns more accurate models in terms of test-set log-likelihood as well as prediction tasks. Furthermore, our tractable learner outperforms intractable models on prediction tasks suggesting that liftable models are a powerful hypothesis space, which may be sufficient for many standard learning problems.},
issn={1573-0565},
doi={10.1007/s10994-015-5532-x},
url={https://doi.org/10.1007/s10994-015-5532-x}
}

@InProceedings{ECML_PROJ,
author="Malhotra, Sagar
and Serafini, Luciano",
editor="Amini, Massih-Reza
and Canu, St{\'e}phane
and Fischer, Asja
and Guns, Tias
and Kralj Novak, Petra
and Tsoumakas, Grigorios",
title="On Projectivity in Markov Logic Networks",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="223--238",
abstract="Markov Logic Networks (MLNs) define a probability distribution on relational structures over varying domain sizes. Like most relational models, MLNs do not admit consistent marginal inference over varying domain sizes i.e. marginal probabilities depend on the domain size. Furthermore, MLNs learned on a fixed domain do not generalize to domains of different sizes. In recent works, connections have emerged between domain size dependence, lifted inference, and learning from a sub-sampled domain. The central idea of these works is the notion of projectivity. The probability distributions ascribed by projective models render the marginal probabilities of sub-structures independent of the domain cardinality. Hence, projective models admit efficient marginal inference. Furthermore, projective models potentially allow efficient and consistent parameter learning from sub-sampled domains. In this paper, we characterize the necessary and sufficient conditions for a two-variable MLN to be projective. We then isolate a special class of models, namely Relational Block Models (RBMs). In terms of data likelihood, RBMs allow us to learn the best possible projective MLN in the two-variable fragment. Furthermore, RBMs also admit consistent parameter learning over sub-sampled domains.",
isbn="978-3-031-26419-1"
}

@book{darwiche2009modeling, 
place={Cambridge}, 
title={Modeling and Reasoning with Bayesian Networks}, 
publisher={Cambridge University Press}, 
author={Darwiche, Adnan}, 
year={2009}
}

@book{murphy2012machine,
  added-at = {2018-04-06T07:06:04.000+0200},
  author = {Murphy, K.P.},
  biburl = {https://www.bibsonomy.org/bibtex/207c8b2bfa20a7d325255a2cf40ca26c4/achakraborty},
  description = {Machine Learning: A Probabilistic Perspective - Kevin P. Murphy - Google Books},
  interhash = {997486e4e92adad73560717a6a07bf82},
  intrahash = {07c8b2bfa20a7d325255a2cf40ca26c4},
  isbn = {9780262018029},
  keywords = {2012 graph-theory machine-learning mit probability textbook},
  lccn = {2012004558},
  publisher = {MIT Press},
  series = {Adaptive computation and machine learning},
  timestamp = {2018-04-06T07:06:04.000+0200},
  title = {Machine Learning: A Probabilistic Perspective},
  url = {https://books.google.co.in/books?id=NZP6AQAAQBAJ},
  year = 2012
}

@book{koller2009probabilistic,
  added-at = {2018-04-06T07:07:25.000+0200},
  author = {Koller, D. and Friedman, N.},
  biburl = {https://www.bibsonomy.org/bibtex/2bfbf27ee9e268b4ad9666fdc925576a2/achakraborty},
  description = {Probabilistic Graphical Models: Principles and Techniques - Daphne Koller, Nir Friedman - Google Books},
  interhash = {0c61213fa4c06778a14fb33e04705fb5},
  intrahash = {bfbf27ee9e268b4ad9666fdc925576a2},
  isbn = {9780262013192},
  keywords = {2009 graph-theory machine-learning mit probability},
  lccn = {2009008615},
  publisher = {MIT Press},
  series = {Adaptive computation and machine learning},
  timestamp = {2018-04-06T07:07:25.000+0200},
  title = {Probabilistic Graphical Models: Principles and Techniques},
  url = {https://books.google.co.in/books?id=7dzpHCHzNQ4C},
  year = 2009
}

@inproceedings{kersting2012lifted,
author = {Kersting, Kristian},
title = {Lifted probabilistic inference},
year = {2012},
isbn = {9781614990970},
publisher = {IOS Press},
address = {NLD},
abstract = {Many AI problems arising in a wide variety of fields such as machine learning, semantic web, network communication, computer vision, and robotics can elegantly be encoded and solved using probabilistic graphical models. Often, however, we are facing inference problems with symmetries and redundancies only implicitly captured in the graph structure and, hence, not exploitable by efficient inference approaches. A prominent example are probabilistic logical models that tackle a long standing goal of AI, namely unifying first-order logic — capturing regularities and symmetries — and probability — capturing uncertainty. Although they often encode large, complex models using few rules only and, hence, symmetries and redundancies abound, inference in them was originally still at the propositional representation level and did not exploit symmetries. This paper is intended to give a (not necessarily complete) overview and invitation to the emerging field of lifted probabilistic inference, inference techniques that exploit these symmetries in graphical models in order to speed up inference, ultimately orders of magnitude.},
booktitle = {Proceedings of the 20th European Conference on Artificial Intelligence},
pages = {33–38},
numpages = {6},
location = {Montpellier, France},
series = {ECAI'12}
}

@article{besag1975statistical,
 ISSN = {00390526, 14679884},
 URL = {http://www.jstor.org/stable/2987782},
 abstract = {A Markovian approach to the specification of spatial stochastic interaction for irregularly distributed data points is reviewed. Three specific methods of statistical analysis are proposed; the first two are generally applicable whilst the third relates only to "normally" distributed variables. Some reservations are expressed and the need for practical investigations is emphasized.},
 author = {Julian Besag},
 journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
 number = {3},
 pages = {179--195},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Statistical Analysis of Non-Lattice Data},
 urldate = {2024-03-15},
 volume = {24},
 year = {1975}
}

@inproceedings{huynh2008discriminative,
author = {Huynh, Tuyen N. and Mooney, Raymond J.},
title = {Discriminative structure and parameter learning for Markov logic networks},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390209},
doi = {10.1145/1390156.1390209},
abstract = {Markov logic networks (MLNs) are an expressive representation for statistical relational learning that generalizes both first-order logic and graphical models. Existing methods for learning the logical structure of an MLN are not discriminative; however, many relational learning problems involve specific target predicates that must be inferred from given background information. We found that existing MLN methods perform very poorly on several such ILP benchmark problems, and we present improved discriminative methods for learning MLN clauses and weights that outperform existing MLN and traditional ILP methods.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {416–423},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@InProceedings{ScaledStructureLearning,
author="Weitk{\"a}mper, Felix
and Ravdin, Dmitriy
and Fabry, Ramona",
editor="Bellodi, Elena
and Lisi, Francesca Alessandra
and Zese, Riccardo",
title="Statistical Relational Structure Learning with Scaled Weight Parameters",
booktitle="Inductive Logic Programming",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="139--153",
abstract="Markov Logic Networks (MLNs) combine relational specifications with probabilistic learning and reasoning. Although they can be specified without reference to a particular domain, their performance across domains of different sizes is generally unfavorable. Domain-size Aware Markov Logic Networks (DA-MLNs) address this issue by scaling down weight parameters based on the domain size. This allows for faster learning by training models on a random sample of the original domain. DA-MLNs also enable transferring models between naturally occurring domains of different sizes. This study proposes a combination of functional gradient boosting and weight scaling for single-target structure learning on large domains. It also evaluates performance and runtime on two benchmark domains of contrasting sizes. The results demonstrate that training a DA-MLN from a sample can significantly reduce learning time for large domains with minor performance trade-offs, which decrease with the size of the original domain. Additionally, the study explores how scaling reacts to varying domain sizes in a synthetic social network domain. It is observed that DA-MLNs outperform unscaled MLNs when the number of connections between individuals grows with domain size, but perform worse when the number of connections remains constant. This justifies the use of unscaled MLNs when sampling isolated subcommunities in areas such as social sciences research.",
isbn="978-3-031-49299-0"
}

@article{ERGM,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2289017},
 abstract = {Log-linear statistical models are used to characterize random graphs with general dependence structure and with Markov dependence. Sufficient statistics for Markov graphs are shown to be given by counts of various triangles and stars. In particular, we show under which assumptions the triad counts are sufficient statistics. We discuss inference methodology for some simple Markov graphs.},
 author = {Ove Frank and David Strauss},
 journal = {Journal of the American Statistical Association},
 number = {395},
 pages = {832--842},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Markov Graphs},
 urldate = {2024-03-20},
 volume = {81},
 year = {1986}
}

@article{Malhotra2023LiftedIB,
  title={Lifted Inference beyond First-Order Logic},
  author={Sagar Malhotra and Davide Bizzaro and Luciano Serafini},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.11738},
  url={https://api.semanticscholar.org/CorpusID:261076456}
}

@inproceedings{LI_Tree,
    title     = {{Lifted Inference with Tree Axioms}},
    author    = {van Bremen, Timothy and Kuželka, Ondřej},
    booktitle = {{Proceedings of the 18th International Conference on Principles of Knowledge Representation and Reasoning}},
    pages     = {599--608},
    year      = {2021},
    month     = {11},
    doi       = {10.24963/kr.2021/57},
    url       = {https://doi.org/10.24963/kr.2021/57},
  }
@misc{chen2024understanding,
      title={Understanding Domain-Size Generalization in Markov Logic Networks}, 
      author={Florian Chen and Felix Weitkämper and Sagar Malhotra},
      year={2024},
      eprint={2403.15933},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{belle2023statistical,
      title={Statistical relational learning and neuro-symbolic AI: what does first-order logic offer?}, 
      author={Vaishak Belle},
      year={2023},
      eprint={2306.13660},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@ARTICLE{Brouard2013-iw,
  title    = "Learning a Markov Logic network for supervised gene regulatory
              network inference",
  author   = "Brouard, C{\'e}line and Vrain, Christel and Dubois, Julie and
              Castel, David and Debily, Marie-Anne and d'Alch{\'e}-Buc,
              Florence",
  abstract = "BACKGROUND: Gene regulatory network inference remains a
              challenging problem in systems biology despite the numerous
              approaches that have been proposed. When substantial knowledge on
              a gene regulatory network is already available, supervised
              network inference is appropriate. Such a method builds a binary
              classifier able to assign a class (Regulation/No regulation) to
              an ordered pair of genes. Once learnt, the pairwise classifier
              can be used to predict new regulations. In this work, we explore
              the framework of Markov Logic Networks (MLN) that combine
              features of probabilistic graphical models with the expressivity
              of first-order logic rules. RESULTS: We propose to learn a Markov
              Logic network, e.g. a set of weighted rules that conclude on the
              predicate ``regulates'', starting from a known gene regulatory
              network involved in the switch proliferation/differentiation of
              keratinocyte cells, a set of experimental transcriptomic data and
              various descriptions of genes all encoded into first-order logic.
              As training data are unbalanced, we use asymmetric bagging to
              learn a set of MLNs. The prediction of a new regulation can then
              be obtained by averaging predictions of individual MLNs. As a
              side contribution, we propose three in silico tests to assess the
              performance of any pairwise classifier in various network
              inference tasks on real datasets. A first test consists of
              measuring the average performance on balanced edge prediction
              problem; a second one deals with the ability of the classifier,
              once enhanced by asymmetric bagging, to update a given network.
              Finally our main result concerns a third test that measures the
              ability of the method to predict regulations with a new set of
              genes. As expected, MLN, when provided with only numerical
              discretized gene expression data, does not perform as well as a
              pairwise SVM in terms of AUPR. However, when a more complete
              description of gene properties is provided by heterogeneous
              sources, MLN achieves the same performance as a black-box model
              such as a pairwise SVM while providing relevant insights on the
              predictions. CONCLUSIONS: The numerical studies show that MLN
              achieves very good predictive performance while opening the door
              to some interpretability of the decisions. Besides the ability to
              suggest new regulations, such an approach allows to
              cross-validate experimental data with existing knowledge.",
  journal  = "BMC Bioinformatics",
  volume   =  14,
  pages    = "273",
  month    =  sep,
  year     =  2013,
  address  = "England",
  language = "en"
}

@inproceedings{DeepProblog, author = {Manhaeve, Robin and Dumancic, Sebastijan and Kimmig, Angelika and Demeester, Thomas and Raedt, Luc De}, title = {DeepProbLog: neural probabilistic logic programming}, year = {2018}, publisher = {Curran Associates Inc.}, address = {Red Hook, NY, USA}, abstract = {We introduce DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques can be adapted for the new language. Our experiments demonstrate that DeepProbLog supports (i) both symbolic and sub-symbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.}, booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems}, pages = {3753–3763}, numpages = {11}, location = {Montr\'{e}al, Canada}, series = {NIPS'18} }


@InProceedings{jaeger,
  author =	{Jaeger, Manfred},
  title =	{{Learning and Reasoning with Graph Data: Neural and Statistical-Relational Approaches}},
  booktitle =	{International Research School in Artificial Intelligence in Bergen (AIB 2022)},
  pages =	{5:1--5:42},
  series =	{Open Access Series in Informatics (OASIcs)},
  ISBN =	{978-3-95977-228-0},
  ISSN =	{2190-6807},
  year =	{2022},
  volume =	{99},
  editor =	{Bourgaux, Camille and Ozaki, Ana and Pe\~{n}aloza, Rafael},
  publisher =	{Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  address =	{Dagstuhl, Germany},
  URL =		{https://drops-dev.dagstuhl.de/entities/document/10.4230/OASIcs.AIB.2022.5},
  URN =		{urn:nbn:de:0030-drops-160035},
  doi =		{10.4230/OASIcs.AIB.2022.5},
  annote =	{Keywords: Graph neural networks, Statistical relational learning}
}

@misc{
marra2020neural,
title={Neural Markov Logic Networks},
author={Giuseppe Marra and Ond{\v{r}}ej Ku{\v{z}}elka},
year={2020},
url={https://openreview.net/forum?id=SkeGvaEtPr}
}

@book{medicine, author = {Natarajan, Sriraam and Kersting, Kristian and Khot, Tushar and Shavlik, Jude}, title = {Boosted Statistical Relational Learners: From Benchmarks to Data-Driven Medicine}, year = {2015}, isbn = {3319136437}, publisher = {Springer Publishing Company, Incorporated}, abstract = {This SpringerBrief addresses the challenges of analyzing multi-relational and noisy data by proposing several Statistical Relational Learning (SRL) methods. These methods combine the expressiveness of first-order logic and the ability of probability theory to handle uncertainty. It provides an overview of the methods and the key assumptions that allow for adaptation to different models and real world applications. The models are highly attractive due to their compactness and comprehensibility but learning their structure is computationally intensive. To combat this problem, the authors review the use of functional gradients for boosting the structure and the parameters of statistical relational models. The algorithms have been applied successfully in several SRL settings and have been adapted to several real problems from Information extraction in text to medical problems. Including both context and well-tested applications, Boosting Statistical Relational Learning from Benchmarks to Data-Driven Medicine is designed for researchers and professionals in machine learning and data mining. Computer engineers or students interested in statistics, data management, or health informatics will also find this brief a valuable resource.} }


@article{ZHANG2020144,
title = {Knowledge graphs completion via probabilistic reasoning},
journal = {Information Sciences},
volume = {521},
pages = {144-159},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520300918},
author = {Richong Zhang and Yongyi Mao and Weihua Zhao},
keywords = {Knowledge graph completion, knowledge graph reasoning, Rule-based inference},
abstract = {Constructing large-scale knowledge base has encountered a bottleneck because of the limitation of natural language processing. Many approaches have been put forward to infer new facts based on existing knowledge. Graph feature models mine rule-like patterns from a knowledge base and use them to predict missing edges. These models take account of the graph structure information and they can explain the existence of a fact reasonably. Existing models only describe local interaction between entities, but how to model co-relationships among facts globally is a tough problem. In this paper, we develop an efficient model which uses association rules to make inferences. First, we use a rule mining model to detect simple association rules and use them to produce large amounts of evidence. Second, based on all the produced evidence and the connections among them, we construct a factor graph which represents the inference space. Then, we develop an EM inference model, wherein the E-step we use Belief Propagation to calculate the marginal distribution of candidate edges and, in the M-step we propose a Generalized Iterative Proportional Fitting algorithm to re-learn the confidence of soft rules. Experiments show that our approach outperforms state-of-the-art approaches in knowledge base completion (KBC) tasks.}
}

@article{ZHANG202114_2,
title = {Neural, symbolic and neural-symbolic reasoning on knowledge graphs},
journal = {AI Open},
volume = {2},
pages = {14-35},
year = {2021},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2021.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666651021000061},
author = {Jing Zhang and Bo Chen and Lingxi Zhang and Xirui Ke and Haipeng Ding},
keywords = {Knowledge graph reasoning, Knowledge graph embedding, Symbolic reasoning, Neural-symbolic reasoning},
abstract = {Knowledge graph reasoning is the fundamental component to support machine learning applications such as information extraction, information retrieval, and recommendation. Since knowledge graphs can be viewed as the discrete symbolic representations of knowledge, reasoning on knowledge graphs can naturally leverage the symbolic techniques. However, symbolic reasoning is intolerant of the ambiguous and noisy data. On the contrary, the recent advances of deep learning have promoted neural reasoning on knowledge graphs, which is robust to the ambiguous and noisy data, but lacks interpretability compared to symbolic reasoning. Considering the advantages and disadvantages of both methodologies, recent efforts have been made on combining the two reasoning methods. In this survey, we take a thorough look at the development of the symbolic, neural and hybrid reasoning on knowledge graphs. We survey two specific reasoning tasks — knowledge graph completion and question answering on knowledge graphs, and explain them in a unified reasoning framework. We also briefly discuss the future directions for knowledge graph reasoning.}
}

@article{natarajan2017markov,
  title={Markov logic networks for adverse drug event extraction from text},
  author={Natarajan, Sriraam and Bangera, Vishal and Khot, Tushar and Picado, Jose and Wazalwar, Anurag and Costa, Vitor Santos and Page, David and Caldwell, Michael},
  journal={Knowledge and information systems},
  volume={51},
  pages={435--457},
  year={2017},
  publisher={Springer}
}

@ARTICLE{social,
  author={Chen, Haiquan and Ku, Wei-Shinn and Wang, Haixun and Tang, Liang and Sun, Min-Te},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Scaling Up Markov Logic Probabilistic Inference for Social Graphs}, 
  year={2017},
  volume={29},
  number={2},
  pages={433-445},
  keywords={Social network services;Markov processes;Probabilistic logic;Knowledge engineering;Graphical models;Cognition;Computer science;Social network analysis;graph pruning;probabilistic graphical models;Markov logic network},
  doi={10.1109/TKDE.2016.2625251}}

@article{zhang2014identifying,
  title={Identifying network public opinion leaders based on Markov logic networks},
  author={Zhang, Weizhe and Li, Xiaoqiang and He, Hui and Wang, Xing and others},
  journal={The scientific world journal},
  volume={2014},
  year={2014},
  publisher={Hindawi}
}

@inproceedings{riedel2009markov,
  title={A markov logic approach to bio-molecular event extraction},
  author={Riedel, Sebastian and Chun, Hong-Woo and Takagi, Toshihisa and Tsujii, Jun’ichi},
  booktitle={Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task},
  pages={41--49},
  year={2009}
}

@article{sakhanenko2010markov,
  title={Markov logic networks in the analysis of genetic data},
  author={Sakhanenko, Nikita A and Galas, David J},
  journal={Journal of Computational Biology},
  volume={17},
  number={11},
  pages={1491--1508},
  year={2010},
  publisher={Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA}
}


@InProceedings{pmlr-v161-bremen21a,
  title = 	 {Faster lifting for two-variable logic using cell graphs},
  author =       {van Bremen, Timothy and Ku\v{z}elka, Ond\v{r}ej},
  booktitle = 	 {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1393--1402},
  year = 	 {2021},
  editor = 	 {de Campos, Cassio and Maathuis, Marloes H.},
  volume = 	 {161},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {27--30 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v161/bremen21a/bremen21a.pdf},
  url = 	 {https://proceedings.mlr.press/v161/bremen21a.html},
  abstract = 	 {We consider the weighted first-order model counting (WFOMC) task, a problem with important applications to inference and learning in structured graphical models. Bringing together earlier work [Van den Broeck et al., 2011, 2014], a formal proof was given by Beame et al. [2015] showing that the two-variable fragment of first-order logic, FO^2, is domain-liftable, meaning it admits an algorithm for WFOMC whose runtime is polynomial in the given domain size. However, applying this theoretical upper bound is often impractical for real-world problem instances. We show how to adapt their proof into a fast algorithm for lifted inference in FO^2, using only off-the-shelf tools for knowledge compilation, and several careful optimizations involving the cell graph of the input sentence, a novel construct we define that encodes the interactions between the cells of the sentence. Experimental results show that, despite our approach being largely orthogonal to that of Forclift [Van den Broeck et al., 2011], our algorithm often outperforms it, scaling to larger domain sizes on more complex input sentences.}
}

@misc{tóth2024complexity,
      title={Complexity of Weighted First-Order Model Counting in the Two-Variable Fragment with Counting Quantifiers: A Bound to Beat}, 
      author={Jan Tóth and Ondřej Kuželka},
      year={2024},
      eprint={2404.12905},
      archivePrefix={arXiv},
      primaryClass={cs.LO}
}

@inproceedings{ijcai2020p591,
  title     = {A Complete Characterization of Projectivity for Statistical Relational Models},
  author    = {Jaeger, Manfred and Schulte, Oliver},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Christian Bessiere},
  pages     = {4283--4290},
  year      = {2020},
  month     = {7},
  note      = {Main track},
  doi       = {10.24963/ijcai.2020/591},
  url       = {https://doi.org/10.24963/ijcai.2020/591},
}

@inproceedings{
jaeger2023a,
title={A Simple Latent Variable Model for Graph Learning and Inference},
author={Manfred Jaeger and Antonio Longa and Steve Azzolin and Oliver Schulte and Andrea Passerini},
booktitle={The Second Learning on Graphs Conference},
year={2023},
url={https://openreview.net/forum?id=S9jem2KZVr}
}

@misc{bellomarini2022swift,
      title={Swift Markov Logic for Probabilistic Reasoning on Knowledge Graphs}, 
      author={Luigi Bellomarini and Eleonora Laurenza and Emanuel Sallinger and Evgeny Sherkhonov},
      year={2022},
      eprint={2210.00283},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{Nickel2015ARO,
  title={A Review of Relational Machine Learning for Knowledge Graphs},
  author={Maximilian Nickel and Kevin P. Murphy and Volker Tresp and Evgeniy Gabrilovich},
  journal={Proceedings of the IEEE},
  year={2015},
  volume={104},
  pages={11-33},
  url={https://api.semanticscholar.org/CorpusID:12161567}
}